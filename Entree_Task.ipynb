{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entree Task: Implementing Your Own Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear Layer \n",
    "Implement the forward and backward functions for a linear layer. Please read the requirement details for Task 1 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, _m, _n, dropout = 0.2):\n",
    "        '''\n",
    "        :param _m: _m is the input X hidden size\n",
    "        :param _n: _n is the output Y hidden size\n",
    "        '''\n",
    "        # \"Kaiming initialization\" is important for neural network to converge. The NN will not converge without it!\n",
    "        self.W = (np.random.uniform(low=-10000.0, high=10000.0, size = (_m, _n)))/10000.0*np.sqrt(6.0/ _m)\n",
    "        self.stored_X = None\n",
    "        self.W_grad = None #record the gradient of the weight\n",
    "        for _ in range(int(self.W.size*0.2)):\n",
    "            self.W[np.random.randint(0, self.W.shape[0])][np.random.randint(0, self.W.shape[1])] = 0\n",
    "        self.b_grad = None #record the gradient of the biases\n",
    "        self.b = (np.random.uniform(low=-10000.0, high=10000.0, size = (1, _n)))/10000.0*np.sqrt(6.0/ _m)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        :param X: shape(X)[0] is batch size and shape(X)[1] is the #features\n",
    "         (1) Store the input X in stored_data for Backward.\n",
    "         (2) :return: X * weights\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        # (1) Store the input X in stored_data for Backward.\n",
    "        self.stored_X = X\n",
    "        # (2) :return: X * weights\n",
    "        return (X @ self.W) + self.b\n",
    "        ##########  Code end   ##########\n",
    "    \n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* shape(output_grad)[0] is batch size and shape(output_grad)[1] is the # output features (shape(weight)[1])\n",
    "         * 1) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **W** and store the product of the gradient and Y_grad in W_grad\n",
    "         * 2) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **X** and return the product of the gradient and Y_grad\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        # 1) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **W** and store the product of the gradient and Y_grad in W_grad\n",
    "        self.W_grad = self.stored_X.T @ Y_grad\n",
    "        # Calculate the gradient of the output (the result of the Forward method) w.r.t. the **b** and store the product of the gradient and Y_grad in b_grad\n",
    "        self.b_grad = np.sum(Y_grad, axis = 0, keepdims = True)\n",
    "        # 2) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **X** and return the product of the gradient and Y_grad\n",
    "        return Y_grad @ self.W.T\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Linear Layer\n",
    "Check your linear forward and backward function implementations with numerical derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[-1.23489461  0.33983695  0.65058254]]\n",
      "Numerical gradient: [[-1.23489461  0.33983695  0.65058254]]\n",
      "Error:  1.2374545832471995e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "m = 6\n",
    "Y_grad = np.random.rand(1, m)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = LinearLayer(n, m)\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backward. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Non-Linear Activation\n",
    "Implement the forward and backward functions for a nonlinear layer. Please read the requirement details for Task 2 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #sigmoid layer\n",
    "    def __init__(self):\n",
    "        self.stored_X = None # Here we should store the input matrix X for Backward\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         *  The input X matrix has the dimension [#samples, #features].\n",
    "         *  The output Y matrix has the same dimension as the input X.\n",
    "         *  You need to perform ReLU on each element of the input matrix to calculate the output matrix.\n",
    "         *  TODO: 1) Create an output matrix by going through each element in input and calculate relu=max(0,x) and\n",
    "         *  TODO: 2) Store the input X in self.stored_X for Backward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.stored_X = X\n",
    "        out = [[max(0, x) for x in features] for features in X]\n",
    "        return np.array(out)\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "         /*  grad_relu(x)=1 if relu(x)=x\n",
    "         *  grad_relu(x)=0 if relu(x)=0\n",
    "         *\n",
    "         *  The input matrix has the name \"output_grad.\" The name is confusing (it is actually the input of the function). But the name follows the convension in PyTorch.\n",
    "         *  The output matrix has the same dimension as input.\n",
    "         *  The output matrix is calculated as grad_relu(stored_X)*Y_grad.\n",
    "         *  TODO: returns the output matrix calculated above\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        grad_relu = np.array([[0 if x <= 0 else 1 for x in features] for features in self.stored_X])\n",
    "        return grad_relu * Y_grad\n",
    "        ##########  Code end   ##########\n",
    "        \n",
    "        \n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.stored_X = None\n",
    "    \n",
    "    def forward(self, X, store=True):\n",
    "        # we use this store variable because backward utilizes the forward\n",
    "        if store:\n",
    "            self.stored_X = X\n",
    "        out = np.array([[1./(1. + np.exp(-x)) for x in features] for features in X])\n",
    "        return out\n",
    "    \n",
    "    def backward(self, Y_grad):\n",
    "        grad_sigmoid = self.forward(self.stored_X, store=False) * (1-self.forward(self.stored_X, store=False))\n",
    "        return grad_sigmoid * Y_grad\n",
    "    \n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.stored_X = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.stored_X = X\n",
    "        out = []\n",
    "        # calculate the softmax activation for each feature in each batch\n",
    "        for feature in X:\n",
    "            exp = np.exp(feature - np.max(feature))\n",
    "            out.append(exp/np.sum(exp))\n",
    "        return np.array(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: ReLU \n",
    "Check your ReLU forward and backward functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[0.57867021 0.58376157 0.88814995]]\n",
      "Numerical gradient: [[0.57867021 0.58376157 0.88814995]]\n",
      "Error:  5.896583221698393e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "Y_grad = np.random.rand(1, n)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = ReLU()\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Loss Function\n",
    "Implement the MSE loss function and its backward derivative. Please read the requirement details for Task 3 in the code comment and in the pdf document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    # cross entropy loss\n",
    "    # return the mse loss mean(y_j-y_pred_i)^2\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stored_diff = None\n",
    "        \n",
    "    def forward(self, prediction, groundtruth):\n",
    "        '''\n",
    "        /*  TODO: 1) Calculate stored_data=pred-truth\n",
    "         *  TODO: 2) Calculate the MSE loss as the squared sum of all \n",
    "         the elements in the stored_data divided by the number of elements, \n",
    "         i.e., MSE(pred, truth) = ||pred-truth||^2 / N, with N as the total \n",
    "         number of elements in the matrix\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        self.stored_diff = prediction - groundtruth\n",
    "        mse = sum(self.stored_diff)**2/len(self.stored_diff)\n",
    "        return mse        \n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    # return the gradient of the input data\n",
    "    def backward(self):\n",
    "        '''\n",
    "        /* TODO: return the gradient matrix of the MSE loss\n",
    "         * The output matrix has the same dimension as the stored_data (make sure you have stored the (pred-truth) in stored_data in your forward function!)\n",
    "         * Each element (i,j) of the output matrix is calculated as \n",
    "         * grad(i,j)=2(pred(i,j)-truth(i,j))/N\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        grad = (2 * self.stored_diff) / len(self.stored_diff)\n",
    "        return grad\n",
    "        ##########  Code end   ##########\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Network Architecture\n",
    "Implement your own neural network architecture. Please read the requirement for Task 4 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers_arch):\n",
    "        '''\n",
    "        /*  TODO: 1) Initialize the array for input layers with the proper feature sizes specified in the input vector.\n",
    "         * For the linear layer, in each pair (in_size, out_size), the in_size is the feature size of the previous layer and the out_size is the feature size of the output (that goes to the next layer)\n",
    "         * In the linear layer, the weight should have the shape (in_size, out_size).\n",
    "         \n",
    "         *  For example, if layers_arch = [['Linear', (256, 128)], ['ReLU'], ['Linear', (128, 64)], ['ReLU'], ['Linear', (64, 32)]],\n",
    "       * \t\t\t\t\t\t\t then there are three linear layers whose weights are with shapes (256, 128), (128, 64), (64, 32),\n",
    "       * \t\t\t\t\t\t\t and there are two non-linear layers.\n",
    "         *  Attention: * The output feature size of the linear layer i should always equal to the input feature size of the linear layer i+1.\n",
    "       */\n",
    "        '''\n",
    "       \n",
    "        ########## Code start  ##########\n",
    "        inp_layers = []\n",
    "        # iterate through each layer in the architecture\n",
    "        for i, layer in enumerate(layers_arch):\n",
    "            layer_obj = None\n",
    "            if layer[0] == \"Linear\" :\n",
    "                # make sure that...\n",
    "                #     two layers out exists (skip the activation layer)\n",
    "                #     and that the input dimensions of the next linear layer matches the output of dim of this layer\n",
    "                if i+2 < len(layers_arch) and layers_arch[i+2][1][0] != layer[1][1]:\n",
    "                    raise Exception(\"Network architecture makes no sense\")\n",
    "                # new linear layer with optional dropout parameter\n",
    "                layer_obj = LinearLayer(layer[1][0], layer[1][1], dropout= 0 if len(layer) < 3 else layer[2])\n",
    "            elif layer[0] == \"ReLU\":\n",
    "                layer_obj = ReLU()\n",
    "            elif layer[0] == \"Sigmoid\":\n",
    "                layer_obj = Sigmoid()\n",
    "            inp_layers.append(layer_obj)\n",
    "            \n",
    "        self.layers = inp_layers\n",
    "        ##########  Code end   ##########\n",
    "        \n",
    "    def recursively_propagate(self, layer, inp):\n",
    "        # if at the last layer\n",
    "        if layer == len(self.layers) - 1:\n",
    "            # return the output of that last layer\n",
    "            return self.layers[layer].forward(inp)\n",
    "        # else recursively propagate to the next layer with the output of this layer\n",
    "        # hence the name\n",
    "        return self.recursively_propagate(layer+1, self.layers[layer].forward(inp))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         * TODO: propagate the input data for the first linear layer throught all the layers in the network and return the output of the last linear layer.\n",
    "         * For implementation, you need to write a for-loop to propagate the input from the first layer to the last layer (before the loss function) by going through the forward functions of all the layers.\n",
    "         * For example, for a network with k linear layers and k-1 activation layers, the data flow is:\n",
    "         * linear[0] -> activation[0] -> linear[1] ->activation[1] -> ... -> linear[k-2] -> activation[k-2] -> linear[k-1]\n",
    "         */\n",
    "        '''\n",
    "        ########## Code start  ##########\n",
    "        return self.recursively_propagate(0, X)\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def recursively_back_prop(self, layer, inp):\n",
    "        # save the result gradient from this layer \n",
    "        res_grad = self.layers[layer].backward(inp)\n",
    "        if layer == 0:\n",
    "            # if at the first layer, just return the result gradient\n",
    "            return res_grad\n",
    "        # recursively propagate backwards with the result gradient\n",
    "        # the chain rule happens within each .backward function of each layer\n",
    "        return self.recursively_back_prop(layer-1, res_grad)\n",
    "    \n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* Propagate the gradient from the last layer to the first layer by going through the backward functions of all the layers.\n",
    "         * TODO: propagate the gradient of the output (we got from the Forward method) back throught the network and return the gradient of the first layer.\n",
    "\n",
    "         * Notice: We should use the chain rule for the backward.\n",
    "         * Notice: The order is opposite to the forward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        return self.recursively_back_prop(len(self.layers)-1, Y_grad)\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Regression Network\n",
    "Check your network implementation with a simple regression task. Here we also provide you a sample implementation for the gradient descent algorithm, which you will find useful for your own Classifier implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor:\n",
    "    #Classifier\n",
    "    def __init__(self, layers_arch, data_function, learning_rate = 1e-3, batch_size = 32, max_epoch = 200):\n",
    "\n",
    "        input_feature_size = 2\n",
    "        output_feature_size = 2\n",
    "\n",
    "        self.train_data = []\n",
    "        self.train_label = []\n",
    "        self.test_data = []\n",
    "        self.test_label = []\n",
    "\n",
    "        self.data_function = data_function\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def dataloader(self):\n",
    "        \n",
    "        '''\n",
    "        We randomly generate the mapping: (x)->(x^3+x^2 + 1)\n",
    "        '''\n",
    "        self.train_data = np.zeros((1000,1))\n",
    "        self.train_label = np.zeros((1000, 1))\n",
    "\n",
    "        for i in range(1000):\n",
    "            self.train_data[i][0] = np.random.uniform(low=0.0, high=10000.0)/10000.0\n",
    "            self.train_label[i][0] = self.data_function(self.train_data[i][0])\n",
    "\n",
    "        self.test_data = np.zeros((200, 1))\n",
    "        self.test_label = np.zeros((200, 1))\n",
    "\n",
    "        for i in range(200):\n",
    "            self.test_data[i][0] = np.random.uniform(low=-0.0, high=10000.0) / 10000.0\n",
    "            self.test_label[i][0] = self.data_function(self.test_data[i][0])\n",
    "\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data)/self.batch_size))\n",
    "\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            batch_label = self.train_label[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            \n",
    "            '''\n",
    "            /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Sample code  ##########\n",
    "            prediction =  self.net.forward(batch_data)\n",
    "            loss += self.loss_function.forward(prediction, batch_label)\n",
    "\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            for i in range(len(self.layers_arch)):\n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "                    self.net.layers[i].b -= self.net.layers[i].b_grad * self.learning_rate\n",
    "            ##########  Sample code ##########\n",
    "            \n",
    "        return loss/n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        prediction = self.net.forward(self.test_data)\n",
    "        loss = self.loss_function.forward(prediction, self.test_label)\n",
    "        return loss\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            train_loss = self.Train_One_Epoch()\n",
    "            test_loss = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", train_loss, \" | Test loss : \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  [48.24432442]  | Test loss :  [173.8111502]\n",
      "Epoch:  2 / 200  | Train loss:  [17.43382817]  | Test loss :  [63.52897653]\n",
      "Epoch:  3 / 200  | Train loss:  [6.31428869]  | Test loss :  [23.60798827]\n",
      "Epoch:  4 / 200  | Train loss:  [2.34123716]  | Test loss :  [9.22936446]\n",
      "Epoch:  5 / 200  | Train loss:  [0.92067106]  | Test loss :  [3.88573425]\n",
      "Epoch:  6 / 200  | Train loss:  [0.39664562]  | Test loss :  [1.79809771]\n",
      "Epoch:  7 / 200  | Train loss:  [0.19400882]  | Test loss :  [0.93684063]\n",
      "Epoch:  8 / 200  | Train loss:  [0.11170158]  | Test loss :  [0.55806711]\n",
      "Epoch:  9 / 200  | Train loss:  [0.07598799]  | Test loss :  [0.37867003]\n",
      "Epoch:  10 / 200  | Train loss:  [0.05927605]  | Test loss :  [0.28762079]\n",
      "Epoch:  11 / 200  | Train loss:  [0.05080487]  | Test loss :  [0.23814022]\n",
      "Epoch:  12 / 200  | Train loss:  [0.0461654]  | Test loss :  [0.20979439]\n",
      "Epoch:  13 / 200  | Train loss:  [0.04344149]  | Test loss :  [0.19254559]\n",
      "Epoch:  14 / 200  | Train loss:  [0.04172098]  | Test loss :  [0.18159001]\n",
      "Epoch:  15 / 200  | Train loss:  [0.04054601]  | Test loss :  [0.17402019]\n",
      "Epoch:  16 / 200  | Train loss:  [0.03967293]  | Test loss :  [0.16799194]\n",
      "Epoch:  17 / 200  | Train loss:  [0.03896171]  | Test loss :  [0.1633665]\n",
      "Epoch:  18 / 200  | Train loss:  [0.03837516]  | Test loss :  [0.158342]\n",
      "Epoch:  19 / 200  | Train loss:  [0.0377357]  | Test loss :  [0.15400245]\n",
      "Epoch:  20 / 200  | Train loss:  [0.03715525]  | Test loss :  [0.14992918]\n",
      "Epoch:  21 / 200  | Train loss:  [0.03663198]  | Test loss :  [0.1461503]\n",
      "Epoch:  22 / 200  | Train loss:  [0.0361256]  | Test loss :  [0.14236152]\n",
      "Epoch:  23 / 200  | Train loss:  [0.0356086]  | Test loss :  [0.13862687]\n",
      "Epoch:  24 / 200  | Train loss:  [0.03514817]  | Test loss :  [0.13525186]\n",
      "Epoch:  25 / 200  | Train loss:  [0.03471375]  | Test loss :  [0.13207259]\n",
      "Epoch:  26 / 200  | Train loss:  [0.03430604]  | Test loss :  [0.12920467]\n",
      "Epoch:  27 / 200  | Train loss:  [0.03392881]  | Test loss :  [0.12638411]\n",
      "Epoch:  28 / 200  | Train loss:  [0.03354726]  | Test loss :  [0.12362772]\n",
      "Epoch:  29 / 200  | Train loss:  [0.03318004]  | Test loss :  [0.12052098]\n",
      "Epoch:  30 / 200  | Train loss:  [0.03280045]  | Test loss :  [0.11779443]\n",
      "Epoch:  31 / 200  | Train loss:  [0.03245154]  | Test loss :  [0.11512307]\n",
      "Epoch:  32 / 200  | Train loss:  [0.03211706]  | Test loss :  [0.11268244]\n",
      "Epoch:  33 / 200  | Train loss:  [0.03180289]  | Test loss :  [0.11015825]\n",
      "Epoch:  34 / 200  | Train loss:  [0.0314736]  | Test loss :  [0.10752634]\n",
      "Epoch:  35 / 200  | Train loss:  [0.03114555]  | Test loss :  [0.10479031]\n",
      "Epoch:  36 / 200  | Train loss:  [0.03081381]  | Test loss :  [0.10220765]\n",
      "Epoch:  37 / 200  | Train loss:  [0.03050409]  | Test loss :  [0.09991268]\n",
      "Epoch:  38 / 200  | Train loss:  [0.03022052]  | Test loss :  [0.09780114]\n",
      "Epoch:  39 / 200  | Train loss:  [0.02994212]  | Test loss :  [0.0954684]\n",
      "Epoch:  40 / 200  | Train loss:  [0.02966076]  | Test loss :  [0.09318759]\n",
      "Epoch:  41 / 200  | Train loss:  [0.02937656]  | Test loss :  [0.090081]\n",
      "Epoch:  42 / 200  | Train loss:  [0.02904899]  | Test loss :  [0.08758989]\n",
      "Epoch:  43 / 200  | Train loss:  [0.02874773]  | Test loss :  [0.08516068]\n",
      "Epoch:  44 / 200  | Train loss:  [0.02848338]  | Test loss :  [0.08308127]\n",
      "Epoch:  45 / 200  | Train loss:  [0.02824602]  | Test loss :  [0.08104752]\n",
      "Epoch:  46 / 200  | Train loss:  [0.02801705]  | Test loss :  [0.07905361]\n",
      "Epoch:  47 / 200  | Train loss:  [0.02779688]  | Test loss :  [0.0772732]\n",
      "Epoch:  48 / 200  | Train loss:  [0.02759556]  | Test loss :  [0.07564609]\n",
      "Epoch:  49 / 200  | Train loss:  [0.02740695]  | Test loss :  [0.07410824]\n",
      "Epoch:  50 / 200  | Train loss:  [0.02722757]  | Test loss :  [0.07264254]\n",
      "Epoch:  51 / 200  | Train loss:  [0.02705524]  | Test loss :  [0.07122502]\n",
      "Epoch:  52 / 200  | Train loss:  [0.02687454]  | Test loss :  [0.06970097]\n",
      "Epoch:  53 / 200  | Train loss:  [0.02670232]  | Test loss :  [0.06828155]\n",
      "Epoch:  54 / 200  | Train loss:  [0.0265396]  | Test loss :  [0.06693904]\n",
      "Epoch:  55 / 200  | Train loss:  [0.02638423]  | Test loss :  [0.06565177]\n",
      "Epoch:  56 / 200  | Train loss:  [0.02623476]  | Test loss :  [0.06440363]\n",
      "Epoch:  57 / 200  | Train loss:  [0.02609011]  | Test loss :  [0.06319166]\n",
      "Epoch:  58 / 200  | Train loss:  [0.02594616]  | Test loss :  [0.06185141]\n",
      "Epoch:  59 / 200  | Train loss:  [0.0257983]  | Test loss :  [0.06060773]\n",
      "Epoch:  60 / 200  | Train loss:  [0.02565924]  | Test loss :  [0.05943082]\n",
      "Epoch:  61 / 200  | Train loss:  [0.02551247]  | Test loss :  [0.05817103]\n",
      "Epoch:  62 / 200  | Train loss:  [0.02537601]  | Test loss :  [0.05700486]\n",
      "Epoch:  63 / 200  | Train loss:  [0.02524798]  | Test loss :  [0.0559022]\n",
      "Epoch:  64 / 200  | Train loss:  [0.02512318]  | Test loss :  [0.05469557]\n",
      "Epoch:  65 / 200  | Train loss:  [0.02499584]  | Test loss :  [0.05358927]\n",
      "Epoch:  66 / 200  | Train loss:  [0.02487722]  | Test loss :  [0.05254969]\n",
      "Epoch:  67 / 200  | Train loss:  [0.02476478]  | Test loss :  [0.0515573]\n",
      "Epoch:  68 / 200  | Train loss:  [0.02465712]  | Test loss :  [0.05059961]\n",
      "Epoch:  69 / 200  | Train loss:  [0.02454616]  | Test loss :  [0.04953435]\n",
      "Epoch:  70 / 200  | Train loss:  [0.02443703]  | Test loss :  [0.04855343]\n",
      "Epoch:  71 / 200  | Train loss:  [0.02433521]  | Test loss :  [0.04762779]\n",
      "Epoch:  72 / 200  | Train loss:  [0.02423847]  | Test loss :  [0.04674294]\n",
      "Epoch:  73 / 200  | Train loss:  [0.02414577]  | Test loss :  [0.04588818]\n",
      "Epoch:  74 / 200  | Train loss:  [0.0240565]  | Test loss :  [0.0450616]\n",
      "Epoch:  75 / 200  | Train loss:  [0.02395804]  | Test loss :  [0.04412736]\n",
      "Epoch:  76 / 200  | Train loss:  [0.02386528]  | Test loss :  [0.04326863]\n",
      "Epoch:  77 / 200  | Train loss:  [0.02377882]  | Test loss :  [0.04246027]\n",
      "Epoch:  78 / 200  | Train loss:  [0.02369684]  | Test loss :  [0.04168769]\n",
      "Epoch:  79 / 200  | Train loss:  [0.02361827]  | Test loss :  [0.0409423]\n",
      "Epoch:  80 / 200  | Train loss:  [0.02354251]  | Test loss :  [0.04021786]\n",
      "Epoch:  81 / 200  | Train loss:  [0.02346904]  | Test loss :  [0.0395129]\n",
      "Epoch:  82 / 200  | Train loss:  [0.02339774]  | Test loss :  [0.03882532]\n",
      "Epoch:  83 / 200  | Train loss:  [0.02332844]  | Test loss :  [0.03815376]\n",
      "Epoch:  84 / 200  | Train loss:  [0.02326098]  | Test loss :  [0.03750035]\n",
      "Epoch:  85 / 200  | Train loss:  [0.02319558]  | Test loss :  [0.03686044]\n",
      "Epoch:  86 / 200  | Train loss:  [0.02313185]  | Test loss :  [0.03623276]\n",
      "Epoch:  87 / 200  | Train loss:  [0.02306963]  | Test loss :  [0.03561868]\n",
      "Epoch:  88 / 200  | Train loss:  [0.02300903]  | Test loss :  [0.03501778]\n",
      "Epoch:  89 / 200  | Train loss:  [0.02294999]  | Test loss :  [0.03442971]\n",
      "Epoch:  90 / 200  | Train loss:  [0.02289247]  | Test loss :  [0.03385414]\n",
      "Epoch:  91 / 200  | Train loss:  [0.02283642]  | Test loss :  [0.03329076]\n",
      "Epoch:  92 / 200  | Train loss:  [0.02278181]  | Test loss :  [0.03273929]\n",
      "Epoch:  93 / 200  | Train loss:  [0.02272849]  | Test loss :  [0.03220125]\n",
      "Epoch:  94 / 200  | Train loss:  [0.02267676]  | Test loss :  [0.03167404]\n",
      "Epoch:  95 / 200  | Train loss:  [0.02262631]  | Test loss :  [0.03115762]\n",
      "Epoch:  96 / 200  | Train loss:  [0.02257711]  | Test loss :  [0.03065186]\n",
      "Epoch:  97 / 200  | Train loss:  [0.02252915]  | Test loss :  [0.03015658]\n",
      "Epoch:  98 / 200  | Train loss:  [0.02248233]  | Test loss :  [0.02967087]\n",
      "Epoch:  99 / 200  | Train loss:  [0.02243669]  | Test loss :  [0.02919551]\n",
      "Epoch:  100 / 200  | Train loss:  [0.02239221]  | Test loss :  [0.02873015]\n",
      "Epoch:  101 / 200  | Train loss:  [0.02234885]  | Test loss :  [0.02827454]\n",
      "Epoch:  102 / 200  | Train loss:  [0.02230657]  | Test loss :  [0.02782842]\n",
      "Epoch:  103 / 200  | Train loss:  [0.02226535]  | Test loss :  [0.02739156]\n",
      "Epoch:  104 / 200  | Train loss:  [0.02222514]  | Test loss :  [0.02696374]\n",
      "Epoch:  105 / 200  | Train loss:  [0.02218576]  | Test loss :  [0.02654562]\n",
      "Epoch:  106 / 200  | Train loss:  [0.02214755]  | Test loss :  [0.02613529]\n",
      "Epoch:  107 / 200  | Train loss:  [0.02211027]  | Test loss :  [0.02573348]\n",
      "Epoch:  108 / 200  | Train loss:  [0.02207391]  | Test loss :  [0.02533996]\n",
      "Epoch:  109 / 200  | Train loss:  [0.02203843]  | Test loss :  [0.02495452]\n",
      "Epoch:  110 / 200  | Train loss:  [0.02200381]  | Test loss :  [0.02457697]\n",
      "Epoch:  111 / 200  | Train loss:  [0.02197002]  | Test loss :  [0.02420713]\n",
      "Epoch:  112 / 200  | Train loss:  [0.02193718]  | Test loss :  [0.02384238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  113 / 200  | Train loss:  [0.02190487]  | Test loss :  [0.02348581]\n",
      "Epoch:  114 / 200  | Train loss:  [0.02187338]  | Test loss :  [0.02313692]\n",
      "Epoch:  115 / 200  | Train loss:  [0.02184266]  | Test loss :  [0.02279536]\n",
      "Epoch:  116 / 200  | Train loss:  [0.02181268]  | Test loss :  [0.02246085]\n",
      "Epoch:  117 / 200  | Train loss:  [0.02178341]  | Test loss :  [0.02213318]\n",
      "Epoch:  118 / 200  | Train loss:  [0.02175482]  | Test loss :  [0.02181215]\n",
      "Epoch:  119 / 200  | Train loss:  [0.0217269]  | Test loss :  [0.02149761]\n",
      "Epoch:  120 / 200  | Train loss:  [0.02169963]  | Test loss :  [0.0211894]\n",
      "Epoch:  121 / 200  | Train loss:  [0.02167298]  | Test loss :  [0.02088738]\n",
      "Epoch:  122 / 200  | Train loss:  [0.02164694]  | Test loss :  [0.02059139]\n",
      "Epoch:  123 / 200  | Train loss:  [0.02162149]  | Test loss :  [0.02030132]\n",
      "Epoch:  124 / 200  | Train loss:  [0.02159661]  | Test loss :  [0.02001702]\n",
      "Epoch:  125 / 200  | Train loss:  [0.02157228]  | Test loss :  [0.01973838]\n",
      "Epoch:  126 / 200  | Train loss:  [0.02154862]  | Test loss :  [0.01946308]\n",
      "Epoch:  127 / 200  | Train loss:  [0.02152526]  | Test loss :  [0.01919393]\n",
      "Epoch:  128 / 200  | Train loss:  [0.02150246]  | Test loss :  [0.01893052]\n",
      "Epoch:  129 / 200  | Train loss:  [0.02148017]  | Test loss :  [0.01867253]\n",
      "Epoch:  130 / 200  | Train loss:  [0.02145834]  | Test loss :  [0.01841924]\n",
      "Epoch:  131 / 200  | Train loss:  [0.02143692]  | Test loss :  [0.01817065]\n",
      "Epoch:  132 / 200  | Train loss:  [0.02141598]  | Test loss :  [0.01792728]\n",
      "Epoch:  133 / 200  | Train loss:  [0.02139549]  | Test loss :  [0.01768886]\n",
      "Epoch:  134 / 200  | Train loss:  [0.02137544]  | Test loss :  [0.01745521]\n",
      "Epoch:  135 / 200  | Train loss:  [0.02135581]  | Test loss :  [0.01722619]\n",
      "Epoch:  136 / 200  | Train loss:  [0.02133658]  | Test loss :  [0.01700166]\n",
      "Epoch:  137 / 200  | Train loss:  [0.02131774]  | Test loss :  [0.01678152]\n",
      "Epoch:  138 / 200  | Train loss:  [0.02129938]  | Test loss :  [0.01656341]\n",
      "Epoch:  139 / 200  | Train loss:  [0.02128124]  | Test loss :  [0.01635028]\n",
      "Epoch:  140 / 200  | Train loss:  [0.0212635]  | Test loss :  [0.01614172]\n",
      "Epoch:  141 / 200  | Train loss:  [0.02124612]  | Test loss :  [0.01593743]\n",
      "Epoch:  142 / 200  | Train loss:  [0.0212291]  | Test loss :  [0.01573722]\n",
      "Epoch:  143 / 200  | Train loss:  [0.02121241]  | Test loss :  [0.01554104]\n",
      "Epoch:  144 / 200  | Train loss:  [0.02119604]  | Test loss :  [0.0153485]\n",
      "Epoch:  145 / 200  | Train loss:  [0.02117991]  | Test loss :  [0.01515993]\n",
      "Epoch:  146 / 200  | Train loss:  [0.02116409]  | Test loss :  [0.01497511]\n",
      "Epoch:  147 / 200  | Train loss:  [0.02114858]  | Test loss :  [0.01479349]\n",
      "Epoch:  148 / 200  | Train loss:  [0.02113339]  | Test loss :  [0.01461553]\n",
      "Epoch:  149 / 200  | Train loss:  [0.02111847]  | Test loss :  [0.01444106]\n",
      "Epoch:  150 / 200  | Train loss:  [0.02110383]  | Test loss :  [0.01426997]\n",
      "Epoch:  151 / 200  | Train loss:  [0.02108944]  | Test loss :  [0.01410215]\n",
      "Epoch:  152 / 200  | Train loss:  [0.02107529]  | Test loss :  [0.01393691]\n",
      "Epoch:  153 / 200  | Train loss:  [0.02106123]  | Test loss :  [0.013775]\n",
      "Epoch:  154 / 200  | Train loss:  [0.02104741]  | Test loss :  [0.01361627]\n",
      "Epoch:  155 / 200  | Train loss:  [0.02103376]  | Test loss :  [0.01345906]\n",
      "Epoch:  156 / 200  | Train loss:  [0.02102039]  | Test loss :  [0.01330531]\n",
      "Epoch:  157 / 200  | Train loss:  [0.02100725]  | Test loss :  [0.01315473]\n",
      "Epoch:  158 / 200  | Train loss:  [0.02099434]  | Test loss :  [0.01300714]\n",
      "Epoch:  159 / 200  | Train loss:  [0.02098164]  | Test loss :  [0.01286178]\n",
      "Epoch:  160 / 200  | Train loss:  [0.02096907]  | Test loss :  [0.01271938]\n",
      "Epoch:  161 / 200  | Train loss:  [0.02095669]  | Test loss :  [0.01257979]\n",
      "Epoch:  162 / 200  | Train loss:  [0.0209445]  | Test loss :  [0.01244287]\n",
      "Epoch:  163 / 200  | Train loss:  [0.02093248]  | Test loss :  [0.01230854]\n",
      "Epoch:  164 / 200  | Train loss:  [0.02092078]  | Test loss :  [0.01217625]\n",
      "Epoch:  165 / 200  | Train loss:  [0.02090924]  | Test loss :  [0.01204657]\n",
      "Epoch:  166 / 200  | Train loss:  [0.02089786]  | Test loss :  [0.01191938]\n",
      "Epoch:  167 / 200  | Train loss:  [0.02088668]  | Test loss :  [0.01179406]\n",
      "Epoch:  168 / 200  | Train loss:  [0.0208756]  | Test loss :  [0.01167125]\n",
      "Epoch:  169 / 200  | Train loss:  [0.02086467]  | Test loss :  [0.01155079]\n",
      "Epoch:  170 / 200  | Train loss:  [0.02085388]  | Test loss :  [0.01143259]\n",
      "Epoch:  171 / 200  | Train loss:  [0.02084323]  | Test loss :  [0.01131659]\n",
      "Epoch:  172 / 200  | Train loss:  [0.02083271]  | Test loss :  [0.0112027]\n",
      "Epoch:  173 / 200  | Train loss:  [0.02082232]  | Test loss :  [0.0110909]\n",
      "Epoch:  174 / 200  | Train loss:  [0.02081204]  | Test loss :  [0.01098112]\n",
      "Epoch:  175 / 200  | Train loss:  [0.02080188]  | Test loss :  [0.01087332]\n",
      "Epoch:  176 / 200  | Train loss:  [0.02079189]  | Test loss :  [0.01076698]\n",
      "Epoch:  177 / 200  | Train loss:  [0.02078199]  | Test loss :  [0.01066271]\n",
      "Epoch:  178 / 200  | Train loss:  [0.0207722]  | Test loss :  [0.01056041]\n",
      "Epoch:  179 / 200  | Train loss:  [0.02076252]  | Test loss :  [0.01045998]\n",
      "Epoch:  180 / 200  | Train loss:  [0.02075293]  | Test loss :  [0.01036138]\n",
      "Epoch:  181 / 200  | Train loss:  [0.02074349]  | Test loss :  [0.01026255]\n",
      "Epoch:  182 / 200  | Train loss:  [0.02073385]  | Test loss :  [0.01016578]\n",
      "Epoch:  183 / 200  | Train loss:  [0.02072436]  | Test loss :  [0.01007096]\n",
      "Epoch:  184 / 200  | Train loss:  [0.02071497]  | Test loss :  [0.00997797]\n",
      "Epoch:  185 / 200  | Train loss:  [0.02070577]  | Test loss :  [0.0098851]\n",
      "Epoch:  186 / 200  | Train loss:  [0.02069664]  | Test loss :  [0.00979443]\n",
      "Epoch:  187 / 200  | Train loss:  [0.02068761]  | Test loss :  [0.00970569]\n",
      "Epoch:  188 / 200  | Train loss:  [0.02067866]  | Test loss :  [0.00961869]\n",
      "Epoch:  189 / 200  | Train loss:  [0.0206698]  | Test loss :  [0.00953331]\n",
      "Epoch:  190 / 200  | Train loss:  [0.020661]  | Test loss :  [0.00944949]\n",
      "Epoch:  191 / 200  | Train loss:  [0.02065236]  | Test loss :  [0.00936643]\n",
      "Epoch:  192 / 200  | Train loss:  [0.02064393]  | Test loss :  [0.00928294]\n",
      "Epoch:  193 / 200  | Train loss:  [0.02063534]  | Test loss :  [0.0092018]\n",
      "Epoch:  194 / 200  | Train loss:  [0.02062684]  | Test loss :  [0.00912258]\n",
      "Epoch:  195 / 200  | Train loss:  [0.02061839]  | Test loss :  [0.00904378]\n",
      "Epoch:  196 / 200  | Train loss:  [0.02060996]  | Test loss :  [0.0089669]\n",
      "Epoch:  197 / 200  | Train loss:  [0.02060158]  | Test loss :  [0.00889129]\n",
      "Epoch:  198 / 200  | Train loss:  [0.02059327]  | Test loss :  [0.00881728]\n",
      "Epoch:  199 / 200  | Train loss:  [0.02058511]  | Test loss :  [0.00874196]\n",
      "Epoch:  200 / 200  | Train loss:  [0.02057689]  | Test loss :  [0.00866891]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00866891])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "# regressor\n",
    "regressor_layers_arch = [['Linear', (1, 16)], ['ReLU'], ['Linear', (16, 16)], ['ReLU'], ['Linear', (16, 1)]]\n",
    "def data_function(x):\n",
    "    return np.power(x,3) + pow(x,2) + 1\n",
    "regressor = Regressor(regressor_layers_arch, data_function, learning_rate = 1e-4, batch_size = 32, max_epoch = 200)\n",
    "regressor.Train()\n",
    "\n",
    "regressor.Test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Classfication Network\n",
    "Implement your own classifier with gradient descent. Please read the requirement for Task 5 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(labels, classes = 10):\n",
    "    '''\n",
    "    /*  Make the labels one-hot.\n",
    "     *  For example, if there are 5 classes {0, 1, 2, 3, 4} then\n",
    "     *  [0, 2, 4] -> [[1, 0, 0, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 1, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 0, 0, 1]]\n",
    "     */\n",
    "    '''\n",
    "    \n",
    "    ########## Code start  ##########\n",
    "    encoded = np.zeros((len(labels),classes))\n",
    "    for i, v in enumerate(labels): \n",
    "        encoded[i][v] = 1\n",
    "    return encoded\n",
    "    ##########  Code end   ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    #Classifier\n",
    "    def __init__(self, train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch, learning_rate = 1e-3, batch_size = 32, max_epoch = 200, classes = 10):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.train_data_path = train_data_path\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.test_labels_path = test_labels_path\n",
    "\n",
    "        self.train_data = [] #The shape of train data should be (n_samples,28^2)\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def dataloader(self):\n",
    "\n",
    "        with open(self.train_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.train_data = np.array(self.train_data)\n",
    "\n",
    "        with open(self.train_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_labels.append(int(line.strip()))\n",
    "        self.train_labels = np.array(self.train_labels)\n",
    "\n",
    "        with open(self.test_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.test_data = np.array(self.test_data)\n",
    "\n",
    "        with open(self.test_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_labels.append(int(line.strip()))\n",
    "        self.test_labels = np.array(self.test_labels)\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data) / self.batch_size))\n",
    "        \n",
    "        softmax = Softmax()\n",
    "        \n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_label = self.train_labels[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_one_hot_label = One_Hot_Encode(batch_label, classes = self.classes)\n",
    "            \n",
    "            '''\n",
    "             /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Code start  ##########\n",
    "            # wrap the output of the last layer in softmax to get discrete probabilities\n",
    "            pred = softmax.forward(self.net.forward(batch_data))\n",
    "            # add the loss\n",
    "            loss += self.loss_function.forward(pred, batch_one_hot_label)          \n",
    "\n",
    "            # calculate the gradient\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            \n",
    "            # go through each layer of the network\n",
    "            for i in range(len(self.layers_arch)):\n",
    "                # if layer type includes trainable params\n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                     # adjust the weights and biases based on the gradient and learning_rate\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "                    self.net.layers[i].b -= self.net.layers[i].b_grad * self.learning_rate\n",
    "            ##########  Code end   ##########\n",
    "        \n",
    "        return loss / n_loop\n",
    "    \n",
    "    def Test(self, record=False):\n",
    "        '''\n",
    "        the class with max score is our predicted label\n",
    "        \n",
    "        added recording option to plot the loss and training accuracies\n",
    "        '''\n",
    "        correct = []\n",
    "        incorrect = []\n",
    "        \n",
    "        softmax = Softmax()\n",
    "        score = softmax.forward(self.net.forward(self.test_data))\n",
    "        accuracy = 0\n",
    "        for i in range(np.shape(score)[0]):\n",
    "            one_label_list = score[i].tolist()\n",
    "            label_pred = one_label_list.index(max(one_label_list))\n",
    "            if label_pred == self.test_labels[i]:\n",
    "                correct.append(self.test_data[i])\n",
    "                accuracy = accuracy +1\n",
    "            else:\n",
    "                incorrect.append(self.test_data[i])\n",
    "\n",
    "        accuracy = accuracy/np.shape(score)[0]\n",
    "        if record:\n",
    "            return (accuracy, correct, incorrect, score, self.test_labels)\n",
    "        else:\n",
    "            return accuracy\n",
    "\n",
    "    def Train(self, record=False):\n",
    "        self.dataloader()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for i in range(self.max_epoch):\n",
    "            loss = self.Train_One_Epoch()\n",
    "            accuracy= self.Test()\n",
    "            if record:\n",
    "                accuracies.append(accuracy)\n",
    "                losses.append(loss)\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", loss, \" | Test Accuracy : \", accuracy)\n",
    "        return accuracies, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "That's it! Congratulations on finishing everything. Now try your network on MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 50  | Train loss:  [0.09468985 0.14240784 0.12268485 0.10337062 0.15424638 0.12448736\n",
      " 0.05167267 0.18818962 0.13326382 0.07933169]  | Test Accuracy :  0.265\n",
      "Epoch:  2 / 50  | Train loss:  [0.0757157  0.15608271 0.05433978 0.09316759 0.11189498 0.09317252\n",
      " 0.03851617 0.16074652 0.10406745 0.07048982]  | Test Accuracy :  0.41\n",
      "Epoch:  3 / 50  | Train loss:  [0.07517932 0.14743684 0.05086752 0.07793804 0.10503157 0.08649553\n",
      " 0.0322565  0.15137175 0.10537587 0.06527159]  | Test Accuracy :  0.525\n",
      "Epoch:  4 / 50  | Train loss:  [0.0722042  0.11916512 0.04810518 0.06725715 0.09910711 0.08295728\n",
      " 0.02661115 0.13172467 0.10159604 0.06313191]  | Test Accuracy :  0.595\n",
      "Epoch:  5 / 50  | Train loss:  [0.05503525 0.08528186 0.04000418 0.05996854 0.09075533 0.07900577\n",
      " 0.02178767 0.10305022 0.0913378  0.05992066]  | Test Accuracy :  0.62\n",
      "Epoch:  6 / 50  | Train loss:  [0.03413875 0.06127918 0.0324235  0.05533813 0.0791746  0.07018835\n",
      " 0.01830983 0.07836721 0.08139364 0.05562817]  | Test Accuracy :  0.655\n",
      "Epoch:  7 / 50  | Train loss:  [0.02132638 0.0471389  0.02563434 0.05080701 0.06688133 0.06271043\n",
      " 0.01494981 0.0606904  0.0753236  0.05096196]  | Test Accuracy :  0.7\n",
      "Epoch:  8 / 50  | Train loss:  [0.01479116 0.03803811 0.02060806 0.04666382 0.05598075 0.05668882\n",
      " 0.01221846 0.0490967  0.06985078 0.04663575]  | Test Accuracy :  0.74\n",
      "Epoch:  9 / 50  | Train loss:  [0.01178554 0.03178701 0.01713278 0.04224589 0.04689259 0.05235258\n",
      " 0.01032387 0.04190813 0.06421889 0.04307786]  | Test Accuracy :  0.75\n",
      "Epoch:  10 / 50  | Train loss:  [0.01000889 0.0273962  0.01461924 0.03802753 0.03981206 0.04882413\n",
      " 0.00890439 0.03720861 0.05848416 0.04007673]  | Test Accuracy :  0.775\n",
      "Epoch:  11 / 50  | Train loss:  [0.00887598 0.02364091 0.01281278 0.03437994 0.03432102 0.04572757\n",
      " 0.00780463 0.03337271 0.05390943 0.03722365]  | Test Accuracy :  0.795\n",
      "Epoch:  12 / 50  | Train loss:  [0.00810271 0.02056556 0.01149288 0.0311513  0.02994008 0.04305184\n",
      " 0.00702955 0.03048136 0.04986225 0.03467455]  | Test Accuracy :  0.81\n",
      "Epoch:  13 / 50  | Train loss:  [0.00762629 0.01814885 0.01043244 0.02857766 0.02663053 0.04088363\n",
      " 0.00644526 0.02824337 0.04652018 0.03227979]  | Test Accuracy :  0.82\n",
      "Epoch:  14 / 50  | Train loss:  [0.00718418 0.01621413 0.00957164 0.02634647 0.02403846 0.03906551\n",
      " 0.00607981 0.0264676  0.04369524 0.0302361 ]  | Test Accuracy :  0.83\n",
      "Epoch:  15 / 50  | Train loss:  [0.00677979 0.01453495 0.00888744 0.02463172 0.02203725 0.03752592\n",
      " 0.00584708 0.02523491 0.04126262 0.02840115]  | Test Accuracy :  0.84\n",
      "Epoch:  16 / 50  | Train loss:  [0.00640784 0.01316854 0.00819179 0.02330132 0.02030934 0.03599298\n",
      " 0.00564515 0.02424161 0.0389088  0.02652222]  | Test Accuracy :  0.84\n",
      "Epoch:  17 / 50  | Train loss:  [0.00608935 0.01203847 0.00756573 0.02218806 0.01890089 0.03465714\n",
      " 0.00549779 0.02349095 0.03691015 0.02510816]  | Test Accuracy :  0.84\n",
      "Epoch:  18 / 50  | Train loss:  [0.00580607 0.01107318 0.00695769 0.02130055 0.01776305 0.03360225\n",
      " 0.0054185  0.02266905 0.03508269 0.0237481 ]  | Test Accuracy :  0.84\n",
      "Epoch:  19 / 50  | Train loss:  [0.00553873 0.01021378 0.00645691 0.02057534 0.01684445 0.03258774\n",
      " 0.00535287 0.02191617 0.03348183 0.02259528]  | Test Accuracy :  0.84\n",
      "Epoch:  20 / 50  | Train loss:  [0.00525364 0.00948536 0.00596398 0.01991624 0.01612284 0.03162452\n",
      " 0.00524404 0.02123789 0.03183947 0.02148586]  | Test Accuracy :  0.84\n",
      "Epoch:  21 / 50  | Train loss:  [0.00501888 0.0088549  0.00552854 0.01928342 0.0154808  0.03042119\n",
      " 0.00517761 0.02079502 0.03042386 0.02055368]  | Test Accuracy :  0.845\n",
      "Epoch:  22 / 50  | Train loss:  [0.00478344 0.00824528 0.0051312  0.01866646 0.01496785 0.02935643\n",
      " 0.00512034 0.02034602 0.02913479 0.01970415]  | Test Accuracy :  0.85\n",
      "Epoch:  23 / 50  | Train loss:  [0.00455334 0.00774551 0.00474815 0.01805264 0.01448457 0.02843062\n",
      " 0.00509091 0.01979925 0.02787189 0.01886019]  | Test Accuracy :  0.85\n",
      "Epoch:  24 / 50  | Train loss:  [0.00433158 0.0072439  0.00441429 0.01752319 0.01404667 0.02738523\n",
      " 0.0050332  0.01942589 0.02658911 0.01817905]  | Test Accuracy :  0.845\n",
      "Epoch:  25 / 50  | Train loss:  [0.00411101 0.00680731 0.00413827 0.01692993 0.01360753 0.02647153\n",
      " 0.00497818 0.01902348 0.02550753 0.01755746]  | Test Accuracy :  0.85\n",
      "Epoch:  26 / 50  | Train loss:  [0.00392962 0.00641271 0.00386404 0.01636626 0.01324594 0.02555073\n",
      " 0.00492656 0.01867065 0.02439353 0.01696194]  | Test Accuracy :  0.85\n",
      "Epoch:  27 / 50  | Train loss:  [0.0037474  0.00605272 0.00364088 0.01586271 0.01287402 0.02466687\n",
      " 0.00487256 0.018342   0.02341153 0.01644193]  | Test Accuracy :  0.85\n",
      "Epoch:  28 / 50  | Train loss:  [0.00357127 0.00569696 0.00342092 0.01538001 0.01254839 0.02384396\n",
      " 0.00481193 0.01792232 0.02249277 0.01585296]  | Test Accuracy :  0.855\n",
      "Epoch:  29 / 50  | Train loss:  [0.00338324 0.00538114 0.00322611 0.01485421 0.01225338 0.02307927\n",
      " 0.00474988 0.01763657 0.02163185 0.01536911]  | Test Accuracy :  0.855\n",
      "Epoch:  30 / 50  | Train loss:  [0.00322028 0.00508785 0.00305147 0.01436486 0.01198195 0.02235067\n",
      " 0.00468364 0.01730187 0.02083611 0.01491594]  | Test Accuracy :  0.85\n",
      "Epoch:  31 / 50  | Train loss:  [0.00306246 0.00481225 0.00288789 0.01388941 0.01166252 0.02160637\n",
      " 0.00462811 0.01695219 0.02008171 0.01443613]  | Test Accuracy :  0.85\n",
      "Epoch:  32 / 50  | Train loss:  [0.00290396 0.00454431 0.00275686 0.01339123 0.01140852 0.02094875\n",
      " 0.0045749  0.01659907 0.01935614 0.01406033]  | Test Accuracy :  0.85\n",
      "Epoch:  33 / 50  | Train loss:  [0.00275313 0.00432176 0.00262904 0.01292658 0.01113851 0.02028658\n",
      " 0.00450492 0.01622569 0.01866014 0.01368592]  | Test Accuracy :  0.85\n",
      "Epoch:  34 / 50  | Train loss:  [0.00261402 0.00409905 0.00251865 0.01248867 0.01089925 0.01963104\n",
      " 0.00442437 0.01591657 0.01797667 0.01332834]  | Test Accuracy :  0.86\n",
      "Epoch:  35 / 50  | Train loss:  [0.00247814 0.00389111 0.00241084 0.01205123 0.01066328 0.0189958\n",
      " 0.00435751 0.01562421 0.01735134 0.01299089]  | Test Accuracy :  0.865\n",
      "Epoch:  36 / 50  | Train loss:  [0.00234578 0.00370594 0.00232974 0.01161978 0.01044816 0.01847755\n",
      " 0.00426956 0.0153378  0.01674672 0.01267053]  | Test Accuracy :  0.87\n",
      "Epoch:  37 / 50  | Train loss:  [0.00223117 0.00351239 0.00224155 0.01121352 0.01021378 0.01786329\n",
      " 0.00419531 0.01500348 0.01617564 0.01233973]  | Test Accuracy :  0.87\n",
      "Epoch:  38 / 50  | Train loss:  [0.00211773 0.00334603 0.00216641 0.01078914 0.0099764  0.01730866\n",
      " 0.0041186  0.01469572 0.01563143 0.0120145 ]  | Test Accuracy :  0.87\n",
      "Epoch:  39 / 50  | Train loss:  [0.00201708 0.00318901 0.00210131 0.01042263 0.00978019 0.01677598\n",
      " 0.00403123 0.01440927 0.01510047 0.01172068]  | Test Accuracy :  0.865\n",
      "Epoch:  40 / 50  | Train loss:  [0.00191746 0.003038   0.00204613 0.01003039 0.00954914 0.01622213\n",
      " 0.00392994 0.0140319  0.01461134 0.01140655]  | Test Accuracy :  0.865\n",
      "Epoch:  41 / 50  | Train loss:  [0.00181666 0.00289291 0.00198432 0.00969095 0.00934879 0.01579548\n",
      " 0.0038443  0.01376962 0.01409115 0.01113651]  | Test Accuracy :  0.865\n",
      "Epoch:  42 / 50  | Train loss:  [0.00173446 0.00274995 0.00193647 0.00927786 0.00912005 0.01525907\n",
      " 0.00375639 0.01340879 0.01360332 0.01083902]  | Test Accuracy :  0.865\n",
      "Epoch:  43 / 50  | Train loss:  [0.00164604 0.00263057 0.00188355 0.00896405 0.00893955 0.01477111\n",
      " 0.00364364 0.01307155 0.01315526 0.01060906]  | Test Accuracy :  0.86\n",
      "Epoch:  44 / 50  | Train loss:  [0.00156881 0.00250865 0.00184249 0.00862715 0.00875706 0.01437159\n",
      " 0.00356421 0.01280697 0.01271311 0.01033378]  | Test Accuracy :  0.86\n",
      "Epoch:  45 / 50  | Train loss:  [0.00148599 0.00240308 0.0018062  0.0082927  0.00854635 0.01389402\n",
      " 0.00345252 0.01246864 0.01226823 0.01009207]  | Test Accuracy :  0.86\n",
      "Epoch:  46 / 50  | Train loss:  [0.00141353 0.00229112 0.00176844 0.00800269 0.00836424 0.01343787\n",
      " 0.003366   0.01216246 0.0118703  0.0098241 ]  | Test Accuracy :  0.86\n",
      "Epoch:  47 / 50  | Train loss:  [0.00134672 0.00220982 0.00174057 0.00771663 0.00815294 0.01305563\n",
      " 0.0032704  0.01188054 0.01146869 0.00956549]  | Test Accuracy :  0.86\n",
      "Epoch:  48 / 50  | Train loss:  [0.00127978 0.00209966 0.00171374 0.00741004 0.00801749 0.01261944\n",
      " 0.00318709 0.01154223 0.01106906 0.00941761]  | Test Accuracy :  0.86\n",
      "Epoch:  49 / 50  | Train loss:  [0.00121591 0.0020163  0.00168113 0.00716133 0.00778667 0.01219078\n",
      " 0.0030729  0.01127359 0.01068602 0.0091102 ]  | Test Accuracy :  0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  50 / 50  | Train loss:  [0.00115611 0.00193458 0.00166064 0.00690375 0.00762338 0.01183836\n",
      " 0.00298447 0.01102078 0.01029787 0.00892114]  | Test Accuracy :  0.86\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "\n",
    "#classifier\n",
    "classifier_layers_arch = [['Linear', (28*28,128), 0.2], \n",
    "                          ['ReLU'], \n",
    "                          ['Linear', (128, 64), 0.2],\n",
    "                          ['ReLU'],\n",
    "                          ['Linear', (64,10)]]\n",
    "\n",
    "cls = Classifier(train_data_path, \n",
    "                 train_labels_path, \n",
    "                 test_data_path, \n",
    "                 test_labels_path, \n",
    "                 layers_arch = classifier_layers_arch, \n",
    "                 learning_rate = 0.01, batch_size =64, max_epoch = 50)\n",
    "\n",
    "accuracies, losses = cls.Train(record=True)\n",
    "accuracy, correct, incorrect, score, test_labels = cls.Test(record=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def vis_acc():\n",
    "    plt.xlabel(\"Test Accuracy\")\n",
    "    plt.ylabel(\"Epochs\")\n",
    "    plt.plot(accuracies)\n",
    "    \n",
    "def vis_loss():\n",
    "    global losses\n",
    "    plt.xlabel(\"Training Loss\")\n",
    "    plt.ylabel(\"Epochs\")\n",
    "    losses = np.array(losses)\n",
    "    for i in range(losses.shape[1]):\n",
    "        plt.plot(losses[:,i])\n",
    "\n",
    "        \n",
    "def vis_confusion_matrix():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    onehot_score = []\n",
    "    for sc in score:\n",
    "        onehot_score.append(np.argmax(sc))\n",
    "    cf_matrix = confusion_matrix(onehot_score, test_labels)\n",
    "    sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "    plt.show()\n",
    "    \n",
    "def vis_examples(num):\n",
    "    print(np.argmax(score[num]))\n",
    "    plt.imshow(correct[num].reshape((28,28)))\n",
    "    \n",
    "        \n",
    "vis_examples(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
